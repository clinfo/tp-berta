# TP-BERTa: A Fundamental LM Adaption Technique to Tabular Data

This repo contains original PyTorch implementation of:

- [Making Pre-trained Language Models Great on Tabular Prediction](https://openreview.net/pdf?id=anzIzGZuLi) (ICLR 2024 Spotlight)

## Key Features

The following key features are proposed in this paper:

- Relative magnitude tokenization (*RMT*), a distributed representation method for continuous values to enhance LM's numerical perception capability.

- Intra-feature attention (*IFA*), a mechanism to pre-fuse feature-wise information for reasonable tabular feature contexts & model acceleration.

- *TP-BERTa*, a resulting LM pre-trained from RoBERTa with the above features for tabular prediction.

## Project Structure

The repo structure and module functions are as follows:

```
â”œâ”€bin ---- // Implementation of tabular models
â”‚â€ƒâ”œâ”€tpberta_modeling.py ---- // TP-BERTa base class
â”‚â€ƒâ””â”€xxx.py ----------------- // Other non-LM DNN baselines
â”œâ”€lib ---- // Utilities
â”‚â€ƒâ”œâ”€aux.py --------------- // Auxiliary Loss: Magnitude-aware Triplet Loss
â”‚â€ƒâ”œâ”€feature_encoder.py --- // Numerical Value Binner (C4.5 discretization)
â”‚â€ƒâ”œâ”€optim.py ------------- // Utilities for optimizer & trainer
â”‚â€ƒâ”œâ”€env.py --------------- // Environment Variables configs
â”‚â€ƒâ”œâ”€data.py -------------- // Dataset & Data Transformation class
â”‚â€ƒâ”œâ”€data_utils.py -------- // Data Config & Multi-task Loader class
â”‚â€ƒâ””â”€xxx.py --------------- // Other standard utils
â”œâ”€data --- // csv file path for pre-training & fine-tuning
â”‚â€ƒâ”œâ”€pretrain-bin
â”‚â€ƒâ”œâ”€pretrain-reg
â”‚â€ƒâ”œâ”€finetune-bin
â”‚â€ƒâ”œâ”€finetune-reg
â”‚â€ƒâ””â”€finetune-mul
â”œâ”€checkpoints --- // Pre-trained model weights & configs (RoBERTa, TP-BERTa)
â”œâ”€configs --- // Model & Training configs for non-LM baselines
â”‚â€ƒâ”œâ”€default --- // default configs
â”‚â€ƒâ””â”€tuned ----- // tuned configs (generated with hyperparameter tuning scripts)
â”œâ”€scripts --- // Experiment codes
â”‚â€ƒâ”œâ”€examples --- // Example shell scripts for main experiments
â”‚â€ƒâ”œâ”€pretrain --- // Codes for TP-BERTa pre-training
â”‚â€ƒâ”œâ”€finetune --- // Codes for baseline fine-tuning & hyperparameter tuning
â”‚â€ƒâ””â”€clean_feat_names.py --- // Text clean for table feature names
```

## Dependencies

All necessary dependencies for TP-BERTa are included in `requirement.txt`. To conduct the packaged baselines, uncomment the corresponding lines.

## TODO

- [ ] Upload pre-trained TP-BERTa checkpoints.

- [ ] Sort and update experiment datasets.

- [ ] Integrate TP-BERTa to HuggingFaceðŸ¤— community.


## Citation

If you find this useful for your research, please cite the following paper:

```
@article{yan2024making,
  title={Making Pre-trained Language Models Great on Tabular Prediction},
  author={Yan, Jiahuan and Zheng, Bo and Xu, Hongxia and Zhu, Yiheng and Chen, Danny and Sun, Jimeng and Wu, Jian and Chen, Jintai},
  journal={arXiv preprint arXiv:2403.01841},
  year={2024}
}
```

## Acknowledgments

Our codes are influenced by the following repos:

- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [RTDL Numerical Embeddings](https://github.com/yandex-research/rtdl-num-embeddings)